## Thread: Conversation with my boss


# Email from my boss on 18 April 2025:
Hi Nima 
 
Thanks for this. Until our new EM starts (offer's gone out, start date TBD), Mt will be acting as your manager so please include him on your emails (after the new EM starts, still would be good for you and Mt to work together super closely). 
 
On your set of questions in the attached deck, they need to be comprehensive and structured. I spent about 5 mins on GenAI and it gave me the following response. It's still not 100% but a good start that you can keep refining with it. Suggest you work on refining this using your own knowledge, as well as using GenAI as much as possible and other research avenues. It can help you with structuring your thinking also. 
 
Thanks! 
M
 
To scope out a “Facebook‑style” real‑time audience‐selection and offer‑ranking engine, you’ll want to frame questions across nine key domains—so that by the end of your discovery you’ll have a clear solution design and an implementation plan. Here’s a structured set of questions your team should ask (and answer):

 

1. Business Objectives & Success Metrics

What uplift targets do we need?
e.g. increase CTR from 0.5% to X%, revenue per mailed customer, incremental profit per campaign.
Which segments or products drive the highest business value?
How will we measure success?
Define primary KPI (CTR, conversion rate, revenue), plus guardrail metrics (unsubscribe rate, complaint rate).
What trade‑offs are acceptable?
e.g. a small drop in open‑rate for a big lift in high‑value conversions.
 

2. Offer Catalog & Prioritization

What offers exist, and how are they currently prioritized?
Is there a hierarchy or mutual exclusivity among offers?
What business rules must apply?
e.g. regulatory blocks, channel‑specific constraints, customer eligibility.
How should competing offers be ranked?
purely by predicted uplift? by a margin of expected value (profit)?
 

 

3. Data Inventory & Quality

What customer data do we have, and at what freshness?
profile, transaction, channel engagement, real‑time events (app/web/mobile).
Are there gaps or quality issues?
missing demographic fields, stale transaction data, siloed systems.
Where will we store features?
batch warehouse vs. real‑time Feature Store.
What privacy/consent constraints apply?
opt‑ins, GDPR/CPRA constraints, retention policies.
 

4. Modeling & Algorithms

Which modeling approaches suit our goals?
multi‑armed bandits (explore/exploit), contextual multi‑armed bandits, reinforcement learning, or ranking models.
How will we benchmark against today’s propensity models?
What input features and temporal windows?
last 24h web events, 30‑day transaction recency/frequency.
How do we handle cold starts (new customers/offers)?
How will we handle model retraining and drift detection?
 
5. Real‑Time Eventing & Personalization

Which events trigger offer evaluation?
session start, deposit posted, login, click stream events.
What latency do we need?
sub‑second? under 1 minute?
What stream‑processing technologies do we adopt?
Kafka, AWS Kinesis, Azure Event Hubs, Spark Streaming.
How do we store and retrieve session context?
 

6. System Architecture & Integration

Where does Pega fit?
will Pega still host orchestration and messaging?
What new components are required?
Feature Store, model‑serving API, ranking service, real‑time event bus.
How will campaigns be triggered and tracked?
What are the data flows—batch vs. streaming?
What SLAs and uptime requirements apply
 

7. Experimentation & A/B Testing

How will we run controlled tests?
hold‑out groups, sequential testing, multi‑arm bandit experiments.
What tooling is needed?
built‑in Pega test flows, external experiment management platforms.
How do we ensure statistical validity?
What reporting cadence and dashboards?
 
8. Deployment, Monitoring & MLOps

What CI/CD pipeline will deploy models?
How will we monitor model performance in production?
online metrics (CTR, conversion), data‑drift, prediction‑drift.
What alerting thresholds?
What rollback strategy?
Who owns model ops vs. data‑engineering vs. Pega‑ops?
9. Governance, Compliance & Risk

What regulatory constraints exist for automated decisioning?
fair lending, anti‑discrimination, consent logging.
How will we audit recommendations?
How do we surface explanations for ranked offers?
What approval workflows must wrap around production launches?


# Reply to my boss on 26 April 2025:

Hi M,
Thanks a lot for sending this.
 
This is a very good guide that covers everything from a high level. Some of these items have been referred to in the deck I sent you before joining (attached again in this email).
What I wanted to share with you in this two-slide deck was the very immediate things I needed to understand in the first two weeks and a very broad and initial ideas that came to my mind.
 
Me and Mt will be working closely together to make a comprehensive plan based on this guide supported by our knowledge and experience.
 
Cheers
Nima

Hi Nima
 
Is the deck you’ve attached still the questions that we need to find out? I don’t really understand what it is sorry.
 
On your point below about wanting to send top of mind questions only, I suggest you always use top-down comms principles:
Always provide a comprehensive big picture view of all the components that need to fir together (e.g., what are all the categories of things you need to find out, e.g., architecture, data sources). In one page you’d show this with a description of what it will entail.
If you don’t give the big picture view and how ALL elements fit together people will get lost. You need to do this especially when you start designing solutions, so people don’t get lost.
You then will deep dive into each area. Whenever you have a table with a list of things, if the list is more than ~3 things you need to categorise them further
If you need to prioritise, you then label the ones that are high priority
 
R actually did the first pass of this well in the questions they sent.
 
I suggest you go through the list of their questions and add any additional ones that you have. I’d also like us  to be super output driven, i.e, any learnings through these sessions that will be conducted we should create their process map etc 

I’d also like us  to be super output driven, i.e, any learnings through these sessions that will be conducted we should create their process map etc so it’s easy to align further with the stakeholders. FYI @Y, @R by the end of the discovery session we should have outputs for:
what’s the current process for creating NBCs & how it will change in each proposed phase?
What’s the current infrastructure, how will the solution plug in to it, any changes needed?
What are the available data that are currently available + what else is readily available, are these enough for the first pass, what else is needed for the other horizons
Etc etc
And all of the above needs to be syndicated and aligned with the relevant SMEs and owners. Once done, I’ll do a final alignment myself with the relevant GMs before taking to EGMs. 
Regards
M


Questions asked by Fr:

Vision and Business Outcomes:

What do you want program to achieve in next 3 months?
What do you want program to achieve in next 6 months?
What is the long-term vision for the NBC capability (1 - 3 years)?
"What business KPIs will this NBC system directly impact? 
(Examples: NPS, product cross-sell rate, churn reduction, cost per acquisition)"
Is the main goal speed, personalization depth, customer value, operational efficiency, consolidation of model or something else?
What are the "must have" outcomes versus "nice to have" outcomes?
Is the vision to have a fully automated system that delivers a Next Best Conversation for each customer across all channels?
How will you measure success of the program? (Example: uplift in conversion, faster time-to-market, improved customer experience)
Are there any pain points or limitations with your current approach that you're looking to address?
Any other considerations for solution approach?
Can GenAI/Agents be considered as part of solution approach?

Model Dev & Scoring

How many NBCs are currently live?
Is there a broad categorization of NBCs available for sharing?
What is the typical process for adding a new NBC - from idea to activation?
What is the typical process for deactivating or removing an NBC from the system?
Who are the key teams or roles involved at each stage of adding, approving, and launching an NBC?
How often are existing NBCs refreshed (e.g., weekly, monthly)?
What types of models / technqiues are used today to generate NBC propensity scores?
Are any LLMs/Agents currently used in NBCs? If yes, which LLMs and which NBCs
How are business outcomes linked back to model performance? (e.g., campaign success attributed to model predictions)
Is there a feedback loop where campaign or NBC outcomes are used to retrain or adjust models?
How are Next Best Conversations (NBCs) currently used in your business? (e.g., always-on campaigns, triggered journeys, inbound personalization, outbound call lists?)
Across which channels are NBCs activated today? (e.g., mobile app, online banking, call center, email, SMS, branch network?)
Are NBCs primarily used for marketing purposes, service interactions, retention, upsell/cross-sell, or other objectives?
How are NBC recommendations currently prioritized if multiple NBCs apply to the same customer? (e.g., based on propensity score, business priority, customer lifecycle stage?)
How do you handle model governance and approvals today?
Are there any regulatory/compliance checks required before a new NBC goes live?
Is the time it takes to add a new Next Best Conversation (NBC) currently a challenge for you in getting to market quickly?
If yes, where do you see the main slowdowns or manual steps today - is it more on the data science side, business approvals, IT integration, or campaign deployment?
Orchestration of NBCs
What is the biggest need for NBC framework
When do you Retrain / Update a model
Are some NBCs powered by triggers
Any additional details?
How are NBC models operationalized and productization?
Does the current process use enterprise LLM APIs?
Are GPUs available to develop NBC solution?

Data:

How is the datalake structured? What are the layers (For example Lending, Raw, Standardized, Curated)
Which layers do NBC solution have access to?
What are the primary data sources feeding into the NBC models? (e.g., transaction data, CRM data, digital engagement, call center interactions?)
Do you have a centralized Customer 360° data platform? If yes, what's the frequency of update
Is there a comprehensive data dictionary available, covering things like table purpose, update frequency, maintainers, primary keys, column types, and column definitions?
Is there a dedicated team or group of people who can help answer data-related queries, or would we need to engage with multiple individual data owners across different domains?
Are any external data sources (such as credit bureau data) currently used by the NBC models?
If yes, could you also share which external sources are being used, and how frequently this data is refreshed or updated?
Is unstructured data — such as call center transcripts, chatbot conversations, customer emails, or social media interactions — currently being used as inputs into the NBC models or decisioning processes?
If yes, could you please share which types of unstructured data are being utilized, and how frequently this data is processed or refreshed?
"Could you help us understand the volume of data involved in the NBC program?
(Specifically:)
Approximately how many tables or datasets are used?
How many customers are covered by the NBC models?
How much historical data is available and typically used (e.g., 24 months, longer)?"
Are all customers already identifitied? Is there a need for identity resolution?
Are all the datasets available at a common location like a data lake or a warehouse? or are they scattered across multiple data sources?
"Do you currently have data quality and assessment reports of your data?
The details in the report would pertain to Exhaustiveness, Accuracy, Mapping and Formats.
Do you have any data quality issues or concerns? (e.g., data completeness, accuracy, consistency, identity resolution, data on legacy systems)"
Will the data for NBC be masked or unmasked?

Infrastructure:

Where is the data currently stored that supports the NBC models and decisioning process?
Can you help us understand how the data supporting NBC models is organized within your AWS environment?
"Can you help us understand the compute environment available today for NBC model development, training, and scoring?
Which platforms or services are primarily used for model training and deployment? (e.g., AWS SageMaker, EMR clusters, EC2 instances, Kubernetes, Databricks, on-prem GPUs?)"
Are different environments maintained for development, testing, and production deployments? (e.g., Dev/Staging/Prod)
Are any enterprise-grade Large Language Models (LLMs) currently available within your environment? (e.g., AWS Bedrock models, Azure OpenAI, Anthropic Claude, proprietary models?)
How can we access data and compute environments? (Remote desktop/ Laptop, VPN etc.)
What version control softwares does your company use? (For example: Github/Gitlab/Bitbucket)
Which other tools are used as part of development and productionalization? (Example - Workflows/Rundeck/JIRA)
"List of accesses which will be required and how long will it take to get each access
(Remote Desktop/Laptop/VPN, Data lake, Compute environment, Confluence)"
Is there any coding/development practice that is mandated?
Fractal would require their accelerator codes to be brought into your environment. What would be the process for this?
Will the development environment be shared or dedicated?

Consumption (PEGA)

Who are the primary users of the NBC system? (e.g., Marketing team, Sales team, Branch staff, Contact centre agents)
Will different business units have different NBC requirements?
Through which platforms are NBCs operationalized today - such as PEGA Customer Decision Hub, Adobe Experience Platform (AEP), Salesforce, or in-house systems?
Can you share details around integration of current NBC integration?
Are there any limitations in current integration with existing crm systems or databases?

Government & Project Management

Are there any templates or formats you prefer project progress to be reported in?
What frequency of communication would you like to establish (e.g., weekly status meetings, daily check-ins)?
What is the time zone and working week (Mon-Fri) for your team that we will be collaborating with on this engagement?
Which communication channels do you prefer for regular updates and discussions (e.g., email, video calls, project management tools)?
Are there regular feedback checkpoints that you'd like to establish throughout the project?
What is the data quality process and frequency? Is the data available for consumption after data quality checks?
Who are the SMEs within your organization who possess domain knowledge relevant to this project?
What is the process to get sign off on the solution from the business teams? How many stakeholders are involved in this process and the success criterias?
Who will be our POC from the Business DS, DE, IT, End consumption teams?

Please generate a list of questions based on the first email from M that excludes questions already asked by Fr.


1. Business Objectives & Success Metrics
How are “value to bank”, “strategic value”, and “value to customer” defined and quantified in the NBC score formula?

What is the end-to-end process to derive each of these components, and how often are they refreshed?

How do different business units prioritize their NBC goals (e.g., revenue, retention, digital engagement)?

What are the business trade-offs between personalization accuracy and operational complexity?

Are there defined tiers or classes of KPIs (e.g., real-time vs. quarterly KPIs)?

2. Offer Catalog & Prioritization
Who owns the master list of offers across channels?

How are offer definitions (eligibility, duration, content) standardized across systems?

Is there a visual or centralized repository of all current offers and their lifecycle status (e.g., active, sunset, in review)?

How do business units track cannibalization or conflicts between concurrent offers?

What are the known pain points in today’s offer governance process?

3. Data Inventory & Quality
What is the process for onboarding new data sources into the NBC pipeline?

Is there a visual data lineage map showing raw → curated → feature-ready layers for NBC?

What is the coverage rate for core customer identifiers across systems (e.g., % of records with persistent customer ID)?

How are source systems monitored for schema drift or feed failures?

What are the rules for data retention and purging for NBC-relevant data?

4. Modeling & Algorithms
What versioning practices are used for existing models (model registry, version control, changelogs)?

Are ensemble models or model stacking strategies currently in use?

How are performance metrics shared across teams – via dashboards, reports, or embedded in tools?

What’s the current process for feature experimentation and validation before model adoption?

Are custom evaluation metrics (beyond standard ROC/AUC) used for any NBC models?

5. Real-Time Eventing & Personalization
Are customer interactions timestamped and stored in a unified event schema?

What determines when an “event” is considered actionable by the NBC engine?

Is there a prioritization mechanism when multiple events occur simultaneously?

What are the rules for suppressing repeat recommendations in real-time (e.g., customer already declined)?

How is real-time context (device, location, previous action) captured and stored?

6. System Architecture & Integration
Is there a current high-level architecture diagram for NBC processes?

What is the integration pathway between model APIs and PEGA orchestration?

Are there internal SLAs between teams for model availability and scoring latency?

How is metadata (e.g., feature source, model name, timestamp) stored alongside predictions?

Is NBC embedded in campaign orchestration tools, or is it consumed externally?

7. Experimentation & A/B Testing
Are business teams trained to interpret test results, or is a centralized analytics team responsible?

Are there visual dashboards that compare NBC models under experimentation side-by-side?

What is the review cadence for testing results (e.g., weekly forums, monthly steering groups)?

What types of experimental results are required to greenlight a model into production?

Is there a hypothesis backlog or experimentation roadmap managed across teams?

8. Deployment, Monitoring & MLOps
Who owns the deployment pipelines for NBC models – business DS, engineering, or platform team?

Are rollback protocols automated or manual for failed NBC deployments?

Is drift monitored separately for feature distributions and model predictions?

Are audit logs maintained for each production model inference (inputs, score, model version)?

How are “early warning indicators” for model degradation currently detected?

9. Governance, Compliance & Risk
Are recommendations from NBCs currently explainable to frontline staff (e.g., agents)?

What internal processes exist to flag high-risk NBCs for additional compliance review?

Are NBC rules reviewed for fairness across customer cohorts (e.g., by age, income, region)?

Is there a clear approval chain and sign-off process for new models and offer logic?

Is there a central governance dashboard showing status of all live and pending NBC models?

Generate a short email with a very polite tone outstanding the following points:
1- I think there is some misunderstanding here, I never wanted to send a comprehensive set of questions but a few most critical questions that I need to find in the next two weeks that help me design my high level plan. Of course there are many questions in my mind and some of which will be answered within the next two weeks or after but due to the fact that I have very limited time, I did not want to put time writing all of them in this stage, however I will in the near future.
2- Your reply to me CCed to Mt and even staff from Fr will imply me weak while I am going to  role as the project lead from CBA. I don't want to criticise your decisions but I wish you had not done that 
3- I do have all the questions in mind and am working on a plan that will address business pain points and aims to achieve outcome in shortest possible time. I will share it with you as soon as it is finished. 


ChatGPT said:
Certainly. Here's a professionally worded and polite email that communicates your message clearly and respectfully:

Subject: Clarification on Initial Deck and Next Steps

Hi M,

Thanks again for your feedback and guidance. I just wanted to clarify a possible misunderstanding: the short deck I shared earlier wasn’t intended to be a comprehensive set of questions. Rather, it focused on a few critical items I need to address in the next two weeks to inform the design of my high-level plan.

Of course, there are many other questions I’m actively thinking about—some of which I expect to answer through discovery over the coming weeks. Given time constraints, I deliberately held off from writing all of them down at this early stage, but I certainly will in the near future.

Also, I wanted to respectfully mention that your response—especially with Mt and colleagues from Fr CC’d—may unintentionally present me as lacking clarity at a time when I am preparing to lead the project on behalf of CBA. I fully appreciate the collaborative nature of our work and your leadership, and I absolutely understand that you meant to help, but I hope you understand my perspective on how it might be perceived.

I want to assure you that I do have a clear direction in mind and am working on a structured plan that addresses business pain points and targets near-term outcomes. I’ll be sharing it with you as soon as it’s finalised.

Thanks again for your support and trust.
Warm regards,
Nima
