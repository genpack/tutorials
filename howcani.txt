How  can I:

## change environmental variables in R environment [27 Dec 2018]:
> Sys.setenv("VAR1" = "value1", "VAR2" = "value2", ...)

## get the value of an environmental variable in R environment [27 Dec 2018]:
> Sys.getenv("VAR1")

## run R command in shell environment [27 Dec 2018]:
Examples:
$ sudo R -e 'install.packages("curl", repos = "http://cran.dev.abc/", lib = "/usr/lib64/R/library/")'
$ R -e 'source("C:/R/myfile.R"); df = read.csv("C:/data/mydata.cdv")'

## install libcurl in redhat [27 Dec 2018]
$ sudo yum -y install libcurl libcurl-devel

## start/stop shinyserver  [27 Dec 2018]:
$ sudo systemctl stop shiny-server
$ sudo systemctl start shiny-server

## find where rpm has installed a package in redhat [27 Dec 2018]:
$ rpm -ql <package_name>

## upload a file to s3 bucket in shell environment [27 Dec 2018]:
$ aws s3 cp <local_address> <s3_address> --profile <your_role>@<your_profile> 
Example:
$ aws s3 cp ~/Documents/data/stky/bigeventlog.csv s3://staging.dmclnt.elsrvcs.com/bigeventlog.csv --profile write@billing

## download a file from s3 bucket in shell environment [27 Dec 2018]:
$ aws s3 cp <s3_address> <local_address> --profile <your_role>@<your_profile> 
Example:
$ aws s3 cp s3://staging.dmclnt.elsrvcs.com/bigeventlog.csv ~/Documents/data/stky/bigeventlog.csv --profile write@billing

## see list of all environmental variables in linux shell [27 Dec 2018]:
$ env

## see the value of an environmental variables in windows shell [22 Jan 2019]:
Example:
$ echo %PATH%


## remotely connect to AWS EMR cluster [27 Dec 2018]:
To connect to an aws EMR cluster:
1- You need to know the uri of your cluster which is referred to as : Master Public DNS
Examples of uri to an EMR cluster:
hadoop@ec2-34-172-254-276.compute-1.amazonaws.com
hadoop@ ec2-67-42-216-27.ap-southeast-2.compute.amazonaws.com
2- You also need a key to open the gate. This key is a small file with extension .pem (or .ppk)
This file contains a RSA256 hashed key for ssh connection:
You need to set permissions for your keys otherwise your connection will come with an error saying the keys are too open and not really secure:
$ chmod 400 ~/mykeypair.pem 
Reading this can help:
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-ssh.html
3 - To connect remotely to the cluster with ssh:
$ ssh hadoop@<EMR URI> -I <your key>
Examples:
$ ssh hadoop@emr.14d1456q-2621-a8ba-fcca-9d15d9f22ae9.stky.elsrvcs.com -i ~/dmclnt.pem

## to copy a folder recursively with all attributes in linux shell [27 Dec 2018]:
$ cp -rp <source_address> <destination_address>
Example:
$ sudo cp -rp smartoptimiser sodc

## copy files from one machine to another using ssh [27 Dec 2018]:
Send:
$ scp <source_address> <username>@<destination_machine_uri>:<destination_address>
Receive:
$ scp <username>@<destination_machine_uri>:<destination_address> <source_address> 
In case the remote machine uses ssh or identity file for authorisation (rather than username & password)
use -i (for identity file) or -F (for ssh key)
Example:
$ sudo scp -i ~/dmclnt.pem config.yml hadoop@emr.79c1756b-2621-a8ba-fcca-8b07d9f22ae9.stky.elsrvcs.com:~/

## get the list of files in a directory in R environment:
> list.files(path = '.')

## publish RStudio Server as a web interface on AWS EMR cluster or EC2 instance [28 Dec 2018]:
1- [remotely connect to] AWS EMR cluster
2- install R on the instance
3- [install Rstudio server] on the instance
4- [make a new user with password]
5- To connect remotely to the app, on each client, you need to [build a tunnel with dynamic port forwarding]
6- [set proxy settings via foxyproxy] on the client with the same port number you build the tunnel trough. Make sure the instance uri will pass the url patterns (regex/wildcard filters) specified in the proxy settings.
7- By default RStudio Server runs on port 8787. To connect, on the client browser: http://<server-ip>:8787
Reading this can help:
https://spark.rstudio.com/examples/yarn-cluster-emr/

## install Rstudio server on any redhat linux machine [28 Dec 2018]:
$ sudo yum update
$ sudo yum install libcurl-devel openssl-devel
$ wget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm
$ sudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm
By default RStudio Server runs on port 8787. To connect, on the client browser: http://<server-ip>:8787

## make a new user with password on linux shell [28 Dec 2018]:
$ sudo useradd -m <username>
$ sudo passwd <username>
enter a new password or change old password

## build a tunnel with dynamic port forwarding to an aws instance [28 Dec 2018]:
$ ssh -i <your_key> -N -D <port_number> <username>@<remote_machine_uri>
Example:
$ ssh -i ~/dmclnt.pem -N -D 8157 hadoop@emr.49c1146b-2411-a8ba-fcca-8b07d9f22ae9.stky.elsrvcs.com
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-connect-master-node-proxy.html

## build a tunnel to a virtual machine instance on the google cloud [24 November 2022]:
$ gcloud compute ssh --project <project> <vm_name> --zone=<zone_location> --tunnel-through-iap
# Example
$ gcloud compute ssh --project scg-analytics-prod ml-temp --zone=australia-southeast1-b --tunnel-through-iap

## map a tunnel to a virtual machine in google cloud into a local host port [24 November 2022]:
# Example
# gcloud compute start-iap-tunnel --project scg-analytics-prod ml-temp 80 --zone=australia-southeast1-b --local-host-port=localhost:8000


## set proxy settings via foxyproxy in google chrome [28 Dec 2018]:
1- install foxyproxy in google chrome
2- go to chrome and right-click on the blue foxyproxy extension icon on the upper right (right after the address bar) --> select options
3- add a new or edit an existing proxy settings (you can either add a new proxy settings manually or import it from a xml file)
3.1- to add a new proxy settings manually:
3.1.1 click on add new proxy
3.1.2 in General tab, select a name and some describing notes and a color for the new proxy settings you are going to add
3.1.3 in Proxy Details tab, manually write all the specifications 
3.1.4 in URL Patterns tab, manually add all the patterns 
3.2- to import proxy settings from xml file:
3.2.1- make a xml text file with any editor. This file can contain multiple proxy settings. The xml file should be like this example:
<?xml version="1.0" encoding="UTF-8"?>
<foxyproxy>
   <proxies>
      <proxy name="emr-socks-proxy" id="2322596116" notes="" fromSubscription="false" enabled="true" mode="manual" selectedTabIndex="2" lastresort="false" animatedIcons="true" includeInCycle="true" color="#0055E5" proxyDNS="true" noInternalIPs="false" autoconfMode="pac" clearCacheBeforeUse="false" disableCache="false" clearCookiesBeforeUse="false" rejectCookies="false">
         <matches>
            <match enabled="true" name="*ec2*.amazonaws.com*" pattern="*ec2*.amazonaws.com*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false" />
            <match enabled="true" name="*ec2*.compute*" pattern="*ec2*.compute*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false" />
            <match enabled="true" name="10.*" pattern="http://10.*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false" />
            <match enabled="true" name="*10*.amazonaws.com*" pattern="*10*.amazonaws.com*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false" />
            <match enabled="true" name="*10*.compute*" pattern="*10*.compute*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false" /> 
            <match enabled="true" name="*.compute.internal*" pattern="*.compute.internal*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false"/>
            <match enabled="true" name="*.ec2.internal* " pattern="*.ec2.internal*" isRegEx="false" isBlackList="false" isMultiLine="false" caseSensitive="false" fromSubscription="false"/>	  
	   </matches>
         <manualconf host="localhost" port="8157" socksversion="5" isSocks="true" username="" password="" domain="" />
      </proxy>
   </proxies>
</foxyproxy>
3.2.2- On the FoxyProxy page, choose Import/Export, choose the xml file you created

## select a set of most relevant features for your ML models [02 Jan 2019]:
Reading these links will help:
https://blog.bigml.com/2014/02/26/smart-feature-selection-with-scikit-learn-and-bigmls-api/
https://github.com/cheesinglee/bigml-feature-subsets
https://arxiv.org/pdf/1812.09044.pdf


## download some dataset for machine learning or other purposes [02 Jan 2019]:
http://archive.ics.uci.edu/ml/index.php

## detect concept drift in machine learning [02 Jan 2019]:
What is concept drift?
https://en.wikipedia.org/wiki/Concept_drift#Datasets
Methods to detect:
http://www.liaad.up.pt/area/jgama/DataStreamsCRC.pdf
http://www.lsi.upc.edu/~abifet/EDDM.pdf

## Read a csv text file [03 Jan 2019]:
Into R data.frame: read.csv(filename, ...)
In Python pandas dataframe: pandas.read_csv(filename, ...)

## add a header with column names to a table [04 Jan 2019]:
Python pandas DataFrame:
data = pandas.DataFrame(data.values, columns = ['col1', 'col2'])

## run spark jobs on emr cluster [06 Jan 2019]: (not complete)
aws emr add-steps --cluster-id <cluster_id> --steps "Type=spark,Name=<job_name>,Args=[--args1,values1,--args2,values2,--args3,values3,...]" --profile <profile>@<role>
Example:
aws emr add-steps --cluster-id j-742YEJR5HYJE --steps Type=spark,Name=PeriodicAggregatorJob,Args=[--deploy-mode,cluster,--master,yarn,--conf,spark.yarn.submit.waitAppCompletion=true,s3://stky.dmclnt.elsrvcs.com/spark_jobs/periodicAggregator.py,s3://stky.dmclnt.elsrvcs.com/configurations/periodicAggregator_config.yml],ActionOnFailure=CONTINUE --profile admin@stky-demo
Read:
https://docs.aws.amazon.com/cli/latest/reference/emr/add-steps.html
https://aws.amazon.com/premiumsupport/knowledge-center/emr-submit-spark-job-remote-cluster/
https://stackoverflow.com/questions/34664090/how-do-i-setup-and-run-sparkr-projects-and-scripts-like-a-jar-file

## deal with categorical variables with large domain in machine learning [07 Jan 2019]: 
Read about fused lasso:
https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels
http://dept.stat.lsa.umich.edu/~jizhu/pubs/Tibs-JRSSB05.pdf
With R: https://www.rdocumentation.org/packages/lqa/versions/1.0-3/topics/fused.lasso
https://www.researchgate.net/publication/265615582_User's_Guide_to_lqa
With Python:

## install a python package from remote repository [08 Jan 2019]:
$ pip install git+<repo_url>
Example:
$ pip install git+https://git-codecommit.ap-southeast-2.amazonaws.com/v1/repos/elib
In Python environment:
>>> !pip install git+<repo_url>

## find address of a variable in R [17 Jan 2019]:
> pryr::address(x)

## find a website to download forex rates [19 Jan 2019]:
https://www.rba.gov.au/statistics/historical-data.html
R package quantmod also gives you API to download, but only the latest 180 days!

## run parallel tasks on sparkR [24 Jan 2019]:
https://databricks.com/session/parallelizing-existing-r-packages-with-sparkr
https://stackoverflow.com/questions/34670006/parallelize-not-working-sparkr
https://blog.rstudio.com/2015/05/28/sparkr-preview-by-vincent-warmerdam/

## list all branches in a git repository from command line [25 Jan 2019]:
$ git branch -a

## switch to a branch in a git repository from command line [25 Jan 2019]:
$ git checkout <branch_name>
Example:
$ git checkout develop

## create a new branch with git [28 Apr 2022]:
$ git checkout -b <branch-name>

## update a branch in the local copy from a git repository from command line [25 Jan 2019]:
$ git pull <repository_name> <branch_name>
Examples:
$ git pull origin develop
$ git pull repo2 master

## dump a python dictionary to a YAML document [29 Jan 2019]:
>>> import yaml
>>> dictionary = {"a": [1, 2], "b": [4, 5]}
>>> print yaml.dump(dictionary)

## run Time-series event-based prediction [06 Feb 2019]:
https://www.researchgate.net/publication/271539746_Time-series_event-based_prediction_An_unsupervised_learning_framework_based_on_genetic_programming

## train LSTM model with R [06 Feb 2019]:
https://www.kaggle.com/taindow/simple-lstm-with-r

## use TensorFlow in R [06 Feb 2019]:
https://tensorflow.rstudio.com/tensorflow/articles/using_tensorflow_api.html

## fit distributions in python with scipy [06 Feb 2019]:
https://docs.scipy.org/doc/scipy/reference/stats.html

## install a python package on windows [09 Feb 2019]:
goto the python folder
Example:
> cd C:\Users\nicolas_\AppData\Local\Programs\Python\Python37
> python -m pip install --upgrade pip
> cd Scripts
> pip install tensorflow
> pip.exe install keras

## Assign multiple new variables on LHS in a single line in R [11 Feb 2019]:
option 1 Example: 
>>> list(x = 12, Y = 'Hello') %>% list2env(.GlobalEnv)
option 2 Example: 
>>> library(zeallot)
>>> c(x, Y) %<-% list(x = 12, Y = 'Hello')


## find list of best R packages [06 Feb 2019]:
https://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html

## see list of files in s3 bucket folder [13 Feb 2019]:
$ aws s3 ls <s3_bucket>/path/to/your/directory/
Example:
$ aws s3 ls s3://em.prd.stcy.wp.els.com/run=283f037c-d5c2-476a-9c5f-4322f72d4173/data/


## find all data types in athena sql [18 Feb 2019]:
https://docs.aws.amazon.com/athena/latest/ug/data-types.html

## find some eventlog sample data for process mining [14 Mar 2019]:
https://data.4tu.nl/repository/collection:event_logs_real

## Get a list of all s3 buckets you have [11 May 2019]:
$ aws s3 ls

## Get a list of all repositories you have in aws codecommit [11 May 2019]:
$ aws codecommit list-repositories

## Get detail information of a codecommit repository in aws [11 May 2019]:
$ aws codecommit get-repository --repository-name <repo_name>

##  install docker on the RHEL machine [30 Jan 2018]:
Example:
[Berta@s029ndpl0703 ~]$sudo yum -y install docker --installroot=/mnt/test/ --nogpgcheck

## to find which versions of each software is available on the RHEL system [30 Jan 2018]:
Examples:
$sudo yum list docker  --showduplicates | sort -r
$sudo yum list R  --showduplicates | sort -r

##  install a specific version of a R package [18 Nov 2022]:
# Example 1:
> packageurl <- "http://cran.r-project.org/src/contrib/Archive/ggplot2/ggplot2_0.9.1.tar.gz"
> install.packages(packageurl, repos=NULL, type="source")
# Example 2:
> devtools::install_version('dbplyr', version = '2.1.1')
# Example 3:
# The remotes package offers an install_version function that can do this directly.
> require(remotes)
> install_version("ggplot2", version = "0.9.1", repos = "http://cran.us.r-project.org")
# You can also manually doenload the specific version and install from command line:
# Example:
$ R CMD INSTALL ggplot2_0.9.1.tar.gz

##  install a specific version of a python package [22 Jan 2020]:
Example:
$ pip install tensorflow~=1.15.0 --user
(--user is required when modification access is denied on other folders)

## run a model in the el pipeline [10 Feb 2020]:
1-pull/checkout/update a local copy of stcky repository:
2-in folder tools look for credentials.yaml and edit it with a text editor
3-enter your username and api-key
4-after setting the credentials, goto tools -> Configs --> Prediction (you can submit other types of job like mlmapper, eventmapper and sampler jobs as well). In each folder, there are some internal folders. Each folder is for a projects(clients). Within each client-folder, there are multiple config-folders with a config.yaml file in it. Open your own config-folder and put your config.yaml file in it.
5-In folder tools, there are jupyter notebook files named as: 01_Obs_Mapper.ipynb, 02_Event_Mapper.ipynb, .... Open file 05_Prediction.ipynb
6-Install all the packages you need. yaml must be version 5.1.2
$pip install --ignore-installed pyyaml==5.1.2
7-Choose client, choose your config file and submit model after verifying 
8-click on get jobs to see the result

## merge a develop branch to the master with git command line [24 Feb 2020]:
# Example:
git checkout branch -> git status -> git add -> git commit -m "something" -> git pull -> git push origin develop -> git checkout master -> git merge develop -> git push origin master
If you want to add a tag: 
# example: (when in master branch) git tag (shows you all the existing tags) -> git tag v5.11.2 -> git push --tags

## delete a tag with git command line [24 Feb 2020]:
# Example: git tag -d v5.9.3

## see all existing tags with git command line [24 Feb 2020]:
$ git tag

## see the commit log on a branch [24 Feb 2020]:
$ git log <branch_name>
#Example:
$ git log master

## to get the value of a config variable with git command line [24 Feb 2020]:
# Example: git config --get user.email

## to set the value of a config variable with git command line [24 Feb 2020]:
# Example: git config user.email nicolas.berta@gmail.com
# If you want to do it on all repositories:
# Example: git config --global user.email nicolas.berta@gmail.com

## install lightgbm python package [25 Feb 2020]:
# pip install lightgbm
# if failed with error: "Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib", then:
# in mac: brew install libomp

## find the version of an installed pythonn package in python environment [25 Feb 2020]:
# Example:
>>> import sklearn
>>> print(sklearn.__version__)

## use pipenv to create a virtualenv for your project and install new packages [02 Mar 2020]:
# install pipenv:
# $ pip install pipenv 
# or in Mac:
# $ brew install pipenv
# go to the root directory of your project and install a package using pipenv:
# Example:
$ cd project_root_dir
$ pipenv install tensorflow~=1.15.0
# to find out where the new environment python is:
$ pipenv shell
$ which python
$ exit

## use virtualenv to create a virtualenv and activate it [28 September 2022]:
# install virtualenv:
# $ pip install virtualenv 
# create your virtual environment:
# $ virtualenv <your-env>
# activate your virtual environment:
# In Mac:
$ source <your-env>/bin/activate
# In Windows:
$ <your-env>\Scripts\activate
# To install a new package in your environment:
$ <your-env>/bin/pip install <your-package>


## to sync your python project with packages in the Pipfile.lock using pipenv [02 Mar 2020]:
# pipenv sync --dev
# This syncs/installs all the project packages in the Pipfile.lock for the project specific python environment


## fix matplotlib backend issue in Mac [02 Mar 2020]:
$ cat > ~/.matplotlib/matplotlibrc
backend: TkAgg

# install a python package from package source [12 Nov 2020]:
$ pip install /path/to/package/folder/
In an environment:
$ cd path/to/environment/folder
$ pipenv run pip install path/to/package/folder/

## test a prediction job in elp
$ cd prediction
$ pipenv install pytest
$ pipenv run pytest -k CASE_XGBOOST_TRAIN_FULL .

## install a python package ignoring the installed version [21 Jan 2021]:
# Example:
$ pip install llvmlite --ignore-installed

## add kernel of global python environment to jupyter notebook   [24 June 2021]:
# In the virtual environment you want to register:
$ cd path/to/folder_name
$ python -m ipykernel install --display-name <NAME> --name <NAME> --user
# or:
# ipython kernel install --name <NAME> --user
# Example:
$ python -m ipykernel install --name dst --display-name dst --user
# if you want to do this with a poetry env you can put poetry run before that command
# then run jupyter notebook and load the notebook file you want to run. In menue bar, 
# go to Kernel --> Change Kernel --> and select the kernel name you specified in <NAME>

## ignore any local changes and just update to the remote master branch in git [29 July 2021]:
$ git fetch
$ git reset --hard origin/master
# If you have local changes that you want to clean, you might have to do the following first:
$ git reset --hard
$ git clean -df 

## get a jb activation code [28 March 2022]:
For now use this:
5UEOFI0I1W-eyJsaWNlbnNlSWQiOiI1VUVPRkkwSTFXIiwibGljZW5zZWVOYW1lIjoiRWx1bGEgR3JvdXAiLCJhc3NpZ25lZU5hbWUiOiJOaW1hIFJhbWV6YW5pIiwiYXNzaWduZWVFbWFpbCI6Im5pbWEucmFtZXphbmlAZWx1bGFncm91cC5jb20iLCJsaWNlbnNlUmVzdHJpY3Rpb24iOiIiLCJjaGVja0NvbmN1cnJlbnRVc2UiOnRydWUsInByb2R1Y3RzIjpbeyJjb2RlIjoiUEMiLCJmYWxsYmFja0RhdGUiOiIyMDIyLTAzLTA1IiwicGFpZFVwVG8iOiIyMDIzLTAzLTA0IiwiZXh0ZW5kZWQiOmZhbHNlfSx7ImNvZGUiOiJQUEMiLCJmYWxsYmFja0RhdGUiOiIyMDIyLTAzLTA1IiwicGFpZFVwVG8iOiIyMDIzLTAzLTA0IiwiZXh0ZW5kZWQiOnRydWV9LHsiY29kZSI6IlBXUyIsImZhbGxiYWNrRGF0ZSI6IjIwMjItMDMtMDUiLCJwYWlkVXBUbyI6IjIwMjMtMDMtMDQiLCJleHRlbmRlZCI6dHJ1ZX0seyJjb2RlIjoiUFNJIiwiZmFsbGJhY2tEYXRlIjoiMjAyMi0wMy0wNSIsInBhaWRVcFRvIjoiMjAyMy0wMy0wNCIsImV4dGVuZGVkIjp0cnVlfSx7ImNvZGUiOiJQQ1dNUCIsInBhaWRVcFRvIjoiMjAyMy0wMy0wNCIsImV4dGVuZGVkIjp0cnVlfV0sIm1ldGFkYXRhIjoiMDEyMDIyMDMwN0NTQUEwMDYwMDkiLCJoYXNoIjoiMzE2NzgyNjUvMTQyNjUzNjY6LTEyNjMxODcyNjMiLCJncmFjZVBlcmlvZERheXMiOjcsImF1dG9Qcm9sb25nYXRlZCI6dHJ1ZSwiaXNBdXRvUHJvbG9uZ2F0ZWQiOnRydWV9-UFfFng4KDOS8QzSEPOYZ9PokPV+dqhCwBhTXRxcd1ePxISWOBZkj0dW8sixc8k0B1vlPC7H7IU/kFuE/tARrtrW9/Gg5LnbIM/m4gOQOZxaQyxdu3whG6SSHaq1ubTaKPMFoEeliwqVoHzBmOjXGJqCE2wb3ny8NBsyhB+q8gTAQ+iSFt9WSFpAfi+Zb4mMPD4iDEQfSwbZgJ4bsULB53QubgJRZX9zkIyWBK+X8350aO55bclS9JETJEgJ6E4NMrKxTVMVtakZVn2hEw2KgJC2ATmGB92k1+naxgI9oV7neRqAaAWdDDoP1Re76aB1n6WGhwTfocknuMdTY99EwXA==-MIIETDCCAjSgAwIBAgIBDTANBgkqhkiG9w0BAQsFADAYMRYwFAYDVQQDDA1KZXRQcm9maWxlIENBMB4XDTIwMTAxOTA5MDU1M1oXDTIyMTAyMTA5MDU1M1owHzEdMBsGA1UEAwwUcHJvZDJ5LWZyb20tMjAyMDEwMTkwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDCP4uk4SlVdA5nuA3DQC+NsEnZS9npFnO0zrmMWcz1++q2UWJNuGTh0rwi+3fUJIArfvVh7gNtIp93rxjtrQAuf4/Fa6sySp4c32MeFACfC0q+oUoWebhOIaYTYUxm4LAZ355vzt8YeDPmvWKxA81udqEk4gU9NNAOz1Um5/8LyR8SGsSc4EDBRSjcMWMwMkYSauGqGcEUK8WhfplsyF61lKSOFA6VmfUmeDK15rUWWLbOMKgn2cxFA98A+s74T9Oo96CU7rp/umDXvhnyhAXSukw/qCGOVhwKR8B6aeDtoBWQgjnvMtPgOUPRTPkPGbwPwwDkvAHYiuKJ7Bd2wH7rAgMBAAGjgZkwgZYwCQYDVR0TBAIwADAdBgNVHQ4EFgQUJNoRIpb1hUHAk0foMSNM9MCEAv8wSAYDVR0jBEEwP4AUo562SGdCEjZBvW3gubSgUouX8bOhHKQaMBgxFjAUBgNVBAMMDUpldFByb2ZpbGUgQ0GCCQDSbLGDsoN54TATBgNVHSUEDDAKBggrBgEFBQcDATALBgNVHQ8EBAMCBaAwDQYJKoZIhvcNAQELBQADggIBAB2J1ysRudbkqmkUFK8xqhiZaYPd30TlmCmSAaGJ0eBpvkVeqA2jGYhAQRqFiAlFC63JKvWvRZO1iRuWCEfUMkdqQ9VQPXziE/BlsOIgrL6RlJfuFcEZ8TK3syIfIGQZNCxYhLLUuet2HE6LJYPQ5c0jH4kDooRpcVZ4rBxNwddpctUO2te9UU5/FjhioZQsPvd92qOTsV+8Cyl2fvNhNKD1Uu9ff5AkVIQn4JU23ozdB/R5oUlebwaTE6WZNBs+TA/qPj+5/wi9NH71WRB0hqUoLI2AKKyiPw++FtN4Su1vsdDlrAzDj9ILjpjJKA1ImuVcG329/WTYIKysZ1CWK3zATg9BeCUPAV1pQy8ToXOq+RSYen6winZ2OO93eyHv2Iw5kbn1dqfBw1BuTE29V2FJKicJSu8iEOpfoafwJISXmz1wnnWL3V/0NxTulfWsXugOoLfv0ZIBP1xH9kmf22jjQ2JiHhQZP7ZDsreRrOeIQ/c4yR8IQvMLfC0WKQqrHu5ZzXTH4NO3CwGWSlTY74kE91zXB5mwWAx1jig+UXYc2w4RkVhy0//lOmVya/PEepuuTTI4+UJwC7qbVlh5zfhj8oTNUXgN0AOc+Q0/WFPl1aw5VV/VrO8FCoB15lFVlpKaQ1Yh+DVU8ke+rt9Th0BCHXe0uZOEmH0nOnH/0onD

## create a python virtual environment with virtualenv [28 March 2022]:
# If you have not installed virtualenv before:
$ pip install --user virtualenv
$ virtualenv myenv
# where myenv can be replaced with the name you want for your virtual environment. 
# The virtual environment can be found in the myenv folder. 
# For Python >= 3.3, you can create a virtual environment with:
$ python -m venv myenv
# After you have created your virtual environment, you can activate the virtual environment with:
$ source myenv/bin/activate
# To deactivate the virtual environment, you can run deactivate. 
# To delete the virtual environment you just need to remove the folder with the virtual environment like: 
$ rm -r myenv

## create a python virtual environment with anaconda [28 March 2022]:
$ conda create -n myenv
# where myenv is the name of your new environment. 
# If you want a specific Python version that is not your current version, you can type:
$ conda create -n myenv python=3.6
# The environment is then stored in the envs folder in your Anaconda directory. 
# After you have created the enviroment, you can activate it by typing:
$ conda activate myenv
# If you now run python, you’ll see that you are in your freshly created virtual environment. 
# To deactivate the environment you can type conda deactivate and you can list all the available environments on 
# your machine with conda env list. 
# To remove an enviroment you can type:
$ conda env remove -n myenv
# After creating your environment, you can install the packages you need besides the one already installed by conda.

## Add Virtual Environment to Jupyter Notebook [28 March 2022]:
# First, make sure your environment is activated
# Second, install ipykernel which provides the IPython kernel for Jupyter if not installed:
$ pip install --user ipykernel
# Next you can add your virtual environment to Jupyter by typing:
$ python -m ipykernel install --user --name=myenv
# This should print the following:
# Installed kernelspec myenv in /home/user/.local/share/jupyter/kernels/myenv

## see which kernels are available in jupyter notebook [28 March 2022]:
$ jupyter kernelspec list
Now, to uninstall/remove the kernel from jupyter notebook [28 March 2022]:
$ jupyter kernelspec uninstall myenv
## uninstall a package in R:
# Example:
> remove.packages("dbplyr")


## change default keyboard shortcuts in pyCharm [28 March 2022]:
# in pycharm go to settings 
# (in Mac in the menu bar left side click on PyCharm beside File --> preferences)
# (in Windows click on the settings icon in the right side of the bar)
# Appearance & Behaviour -> Keymap
# Choose your action and change key. For example for running selected code in console and 
# change it from Alt + Shift + E to ctrl + enter (like R Studio):
# Settings -> Keymap - Other -> Execute selection in Python Console --> add keyboard shortcut
# and just type your desired shortcut (Ctrl + Enter)
# You can keep the old shortcut or remove it if you like

## download sample eventlog for prediction
# https://www.kaggle.com/code/bechorfamedelamine/customer-behaviour-and-product-efficiency/data?select=2019-Oct.csv
 
## find non UTF-8 encoding in a script file by R Studio [14 April 2022]:
# go to Edit -> Find (Ctrl + F) and search for [^\x00-\x7F] with enabled Regex field in the search bar

## How Felix fixed my pipx and aws issue in Mac (order of execution is bottom to top) [28 April 2022]:
% curl -sSL https://install.python-poetry.org | python3
% pip3 uninstall awscli
% which aws
% python3 -m pipx ensurepath
$ which pipx
$ python3 -m pip install pipx
% python3 -m pip install aws-mfa
% which python3
% python3 --version
% python --version
% sudo nano /etc/paths
% pip3 install aws-mfa
% which pip3
% which pip
% rm $(which aws-mfa)
% echo $PATH
% which aws-mfa
% pipx install jobwrangler
% pip3 show aws-mfa 
% pyenv install 3.7.12
% xcode-select --install
% echo 'export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.zprofile
% echo 'eval "$(pyenv init --path)"' >> ~/.zprofile
% exec $SHELL
% curl https://pyenv.run | bash
% python3 --version
% which pip
% softwareupdate --all --install --force
% pyenv install 3.7.13 
% pyenv install 3.7.9
% which awscli
% pip3 install awscli --user
% which aws
% rm /usr/local/bin/aws
% which aws
% python3 -m pipx ensurepath
% python3 -m pip install pipx
% python3 -m pip install aws-mfa



## test equality of two pandas dataframes [2 May 2022]:
# Example:
>>> pd.testing.assert_frame_equal(output, expected_output)

## Move a committed change into a new branch [10 May 2022]:
$ git branch newbranch      # Create a new branch, saving the desired commits
$ git checkout master       # checkout master, this is the place you want to go back
$ git reset --hard HEAD~3   # Move master back by 3 commits (Make sure you know how many commits you                               # need to go back)
$ git checkout newbranch    # Go to the new branch that still has the desired commits

## Install apache-airflow [18 Sep 2022]:
# 1. Install Python3 and pip (Ignore this step if you have already installed)
#
% brew install python
% python --version
% curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
% python get-pip.py
# To install Airflow, make sure the pip version is 20.2.4
% pip install --upgrade pip==20.2.4
#
# 2. [Optional] Create a Virtual Environment Install Airflow
% pip install virtualenv
% virtualenv -p python venv
# Now activate the virtual environment so that all packages get install onto the virtual  environment rather than your computer.
% source venv/bin/activate
#  
# 3.Install Airflow in a new airflow directory
% mkdir airflow && cd airflow
% pip install apache-airflow
# Setup the proper directory structure and create a new airflow folder. First get the path to the # airflow folder with pwd and then export that as the airflow home directory to that path.
% pwd

## to save github access token to avoid entering username and password each time you push/pull [12 December 2022]: 
# Saving tokens in Mac:
# You’ll need the OSX keychain credential helper. If you installed Git using Homebrew or via the Xcode command line, then osxkeychain helper should be part of the installation.
# You can check for its installation via the following command:
$ git credential-osxkeychain
# To have Git store the token in osxkeychain, type:
$ git config --global credential.helper osxkeychain
# The next time you are prompted for a user name and password, simply type your GitHub account name, then your personal access token. 
# This will be a one time operation after which the token will be permanently stored.
# Saving tokens in Windows:
# If you are using a Windows based Git application, you might see a window pop-up when you are first asked to enter your token.
# Fpr other operaing systems look at this guide:
# https://mgimond.github.io/Colby-summer-git-workshop-2021/authenticating-with-github.html

## install google cloud [20 September 2022]:
# download google-cloud from here:
# https://cloud.google.com/sdk/docs/install
# A .tar.gz file is downloaded. Uncompress and it creates a folder named: google-cloud-sdk
% ./google-cloud-sdk/install.sh
# Open a new terminal so that the changes take effect
# Initialize the gcloud CLI:
% ./google-cloud-sdk/bin/gcloud init
# Choose your project and continue
# To use google python package,
# https://cloud.google.com/sdk/gcloud/reference/topic/startup
# If you have multiple Python interpreters available (including a bundled python) or if you don't have one on your PATH, # you can specify which interpreter to use by setting the CLOUDSDK_PYTHON environment variable. 
# Examples:
# Use the python3 interpreter on your path
% export CLOUDSDK_PYTHON=python3
# Use a python you have installed in a special location
% export CLOUDSDK_PYTHON=/usr/local/my-custom-python-install/python
# if you wanted to change your active project for just one terminal you could run:
% export CLOUDSDK_CORE_PROJECT=my-project
# Installing the client library:
# pip install --upgrade google-cloud-bigquery
# Continue from here:
# https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python

## login with gcloud cli [28 September 2022]:
$ gcloud auth login

## See list of killed processes on a linux machine:
$ dmesg -T | egrep -i 'killed process'

## Run a process(job) in a linux machine and keep running even after the terminal is closed:
# Example:
$ python script.py &
# There is also the nohup command. If you start with nohup like:
$ nohup python script.py &
# Then it redirects the standard output, meaning any printouts you had in the code will be automatically written to a file called nohup.out
# You can also select your own log file:
$ nohup </path/to/command-name> arg1 arg2 > myoutput.log &
# exit: after nohup and & start the command in the background, you will still have to type exit or CTRL-D to go back to bash cursor:
$ nohup python script.py arg1 > script.log &
$ exit

## resolve machine incompatible error [20 October 2022]:
# This error happens when a python package cannot be imported with reticulate in R and you see this error:
# "(mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))"
# In python:
>>> import platform
>>> is_x86 = platform.machine() in ("i386", "AMD64", "x86_64")
# If is_x86 is False:
# pip install --upgrade pip
% pip uninstall <your_package>
% arch -x86_64 pip install <your_package>

## to find out what python binary executables are available on your system [20 October 2022]:
$ ls /usr/bin/python*
# /usr/bin/python  /usr/bin/python2  /usr/bin/python2.7  /usr/bin/python3  /usr/bin/python3.4  /usr/bin/python3.4m  /usr/bin/python3m
# To check what is your default python version execute:
$ python --version
# Python 2.7.8

## get the version of all packages in your environment [20 October 2022]:
$ python -m pip freeze

## find a good article for bigquery on google cloud [25 October 2022]:
# Good links:
# https://calogica.com/r/bigquery/2020/08/18/r-bigquery.html
# https://github.com/r-dbi/bigrquery/issues/474
# https://stackoverflow.com/questions/64792914/how-to-write-virtual-bq-table-back-to-bq-using-r-dbi-and-bigrquery
# https://cran.r-project.org/web/packages/googleCloudStorageR/vignettes/googleCloudStorageR.html


## git push to a repository under a different username [08 November 2022]:
% git config --local credential.helper ""
% git push
# After the push command, username and password are asked


## install R on on Ubuntu 20.04 [16 November 2022]:
# Install the dependencies necessary to add a new repository over HTTPS:
$ sudo apt install dirmngr gnupg apt-transport-https ca-certificates software-properties-common
# Add the CRAN repository to your system sources’ list:
$ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
$ sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/'
# install R
$ sudo apt install r-base
$ R --version

## Create Docker Image for GCP VM:
# In a working directory, we need to have two files: dockerfile.txt, bootstrap.sh
# Write dockerfile.txt. It is a text file that contains all commands needed to build a given iamge. 
# Following is the sample contents of the dockerfile.txt:

# Specify the parent image from which we build.
# in this example, we are going to build R-studio server
FROM rocker/tidyverse:latest
# Install commonly used dependencies
RUN apt-get --allow-releaseinfo-change update
RUN apt-get install -y python3-pip
RUN pip3 install requests requests-oauthlib itsdangerous
RUN apt-get install -y r-base
RUN apt-get install nano
RUN apt-get install -y texlive-latex-base texlive-latex-recommended texlive-fonts-recommended
# Install and set up google cloud sdk -- This is needed to read/write from google cloud storage
RUN apt-get install -y curl gnupg apt-transport-https ca-certificates && \
    echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && \
    apt-get update -y && \
    apt-get install google-cloud-sdk -y
# clean up
RUN apt-get clean \
&& rm -rf /var/lib/apt/lists/ \
&& rm -rf /tmp/downloaded_packages/ /tmp/*.rds



# r setup -- install R packages that your application requires
# RUN R -e "install.packages('REQUIRED PACKAGES', repos = 'http://cran.us.r-project.org')"
# For instance,
RUN R -e "install.packages('pacman', repos = 'http://cran.us.r-project.org')"

# Add Home directory to Bash
RUN echo 'export PATH="$HOME/bin:$PATH"' >> /root/.bashrc

# Copy bootstrap shell script to root COPY ./bootstrap.sh /root# Make the script executable
RUN chmod +x /root/bootstrap.sh # Run bootsctrap.shENTRYPOINT [ "/bin/bash", "/root/bootstrap.sh" ]
Write bootstrap.sh Bash shell Script. In the below script, $CAPITAL is a environment variable that you have to set when deploying VM machine. 
#Telling computer that we execute shell script#!/bin/bash # make required directorymkdir -p /root/exports
mkdir -p /root/imports # Copy task shell script from gcs and run task scriptgsutil cp $GCS_WORKSPACE/$GCS_WORKSPACE_SH ./$GCS_WORKSPACE_SH # Make the script executablechmod +x ./$GCS_WORKSPACE_SH # Start shell scriptsource ./$GCS_WORKSPACE_SH
Start Docker desktop
Open terminal and get to the working directory and type. DOCKER_IMAGE_NAME should be understandable to your team members.
docker build -t DOCKER_IMAGE_NAME -f dockerfile.txt .
Then change docker tag. PROJECT_ID is the google cloud project id to which you want to deploy your docker image
docker tag DOCKER_IMAGE_NAME gcr.io/PROJECT_ID/DOCKER_IMAGE_NAME
